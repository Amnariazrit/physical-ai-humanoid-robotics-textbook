"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[4430],{3673:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter1","label":"Introduction to ROS 2: Overview and Architecture","docId":"module1-ros2/chapter1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter2","label":"ROS 2 Nodes and Topics: Communication in Robotics","docId":"module1-ros2/chapter2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter3","label":"ROS 2 Services and Actions: Synchronous and Asynchronous Calls","docId":"module1-ros2/chapter3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter4","label":"Python Integration: Bridging Python Agents to ROS 2","docId":"module1-ros2/chapter4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter5","label":"URDF for Humanoids: Robot Description and Simulation Basics","docId":"module1-ros2/chapter5","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter1"},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter1","label":"Gazebo Basics: Environment Setup and Physics Simulation","docId":"module2-gazebo/chapter1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter2","label":"Physics Simulation: Gravity, Collisions, Dynamics","docId":"module2-gazebo/chapter2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter3","label":"URDF and SDF Formats: Robot and Environment Descriptions","docId":"module2-gazebo/chapter3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter4","label":"Unity Visualization: Complementing Gazebo Simulations","docId":"module2-gazebo/chapter4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter5","label":"Sensor Simulation: LiDAR, Depth Camera, IMU","docId":"module2-gazebo/chapter5","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter1"},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter1","label":"Isaac Sim Introduction: Overview and Capabilities","docId":"module3-isaac/chapter1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter2","label":"AI-powered Perception: Object Detection, Depth Perception","docId":"module3-isaac/chapter2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter3","label":"Isaac ROS: Hardware-Accelerated SLAM and Navigation","docId":"module3-isaac/chapter3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter4","label":"Reinforcement Learning: Training Humanoid Behavior","docId":"module3-isaac/chapter4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter5","label":"Sim-to-Real Transfer: Deploying Models to Real Robots","docId":"module3-isaac/chapter5","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter1"},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1","label":"Voice-to-Action with Whisper: Capturing and Interpreting Commands","docId":"module4-vla/chapter1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2","label":"Cognitive Planning: LLM Planning of Sequences","docId":"module4-vla/chapter2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter3","label":"ROS 2 Action Sequencing: Executing Planned Actions","docId":"module4-vla/chapter3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter4","label":"Multi-modal Interaction: Speech, Gesture, Vision Integration","docId":"module4-vla/chapter4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter5","label":"Capstone Integration: Autonomous Humanoid Demonstration","docId":"module4-vla/chapter5","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1"}]},"docs":{"module1-ros2/chapter1":{"id":"module1-ros2/chapter1","title":"Introduction to ROS 2: Overview and Architecture","description":"The Robot Operating System (ROS) is a flexible framework for writing robot software. It\'s a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms. ROS 2 is the successor to the original ROS (now often referred to as ROS 1), re-engineered to address the needs of modern robotics applications, including real-time performance, multi-robot systems, and embedded device support.","sidebar":"tutorialSidebar"},"module1-ros2/chapter2":{"id":"module1-ros2/chapter2","title":"ROS 2 Nodes and Topics: Communication in Robotics","description":"In the previous chapter, we introduced the fundamental concepts of ROS 2 architecture, including nodes and topics. This chapter will delve deeper into these two core elements, providing practical examples of how they facilitate communication within a robotic system using Python.","sidebar":"tutorialSidebar"},"module1-ros2/chapter3":{"id":"module1-ros2/chapter3","title":"ROS 2 Services and Actions: Synchronous and Asynchronous Calls","description":"In the previous chapter, we explored ROS 2 topics, which provide an asynchronous, one-way communication mechanism ideal for continuous data streams. This chapter introduces two other critical communication patterns: Services for synchronous request/reply interactions, and Actions for asynchronous, long-running tasks with feedback and preemption capabilities.","sidebar":"tutorialSidebar"},"module1-ros2/chapter4":{"id":"module1-ros2/chapter4","title":"Python Integration: Bridging Python Agents to ROS 2","description":"Python is a widely used language in robotics due to its simplicity, extensive libraries, and rapid prototyping capabilities. ROS 2 provides excellent support for Python through its client library, rclpy, allowing developers to write powerful and flexible ROS 2 nodes and integrate various Python-based AI agents and algorithms into their robotic systems.","sidebar":"tutorialSidebar"},"module1-ros2/chapter5":{"id":"module1-ros2/chapter5","title":"URDF for Humanoids: Robot Description and Simulation Basics","description":"The Unified Robot Description Format (URDF) is an XML format used in ROS to describe the physical characteristics of a robot. It allows you to define the robot\'s kinematics (links and joints), visual properties (geometry and colors), and collision properties (shapes for physics simulation). For humanoid robots, URDF is crucial for accurately representing their complex structure, enabling simulation, visualization, and motion planning.","sidebar":"tutorialSidebar"},"module2-gazebo/chapter1":{"id":"module2-gazebo/chapter1","title":"Gazebo Basics: Environment Setup and Physics Simulation","description":"Gazebo is a powerful 3D robotics simulator that allows you to accurately test algorithms, design robots, and perform complex scenarios in a virtual environment. It\'s widely used in the robotics community and integrates seamlessly with ROS 2, making it an indispensable tool for developing and validating robotic systems.","sidebar":"tutorialSidebar"},"module2-gazebo/chapter2":{"id":"module2-gazebo/chapter2","title":"Physics Simulation: Gravity, Collisions, Dynamics","description":"Gazebo\'s ability to accurately simulate physical interactions is central to its utility in robotics development. This chapter will delve deeper into the core components of physics simulation within Gazebo, covering how to configure gravity, understand and manage collisions, and work with the dynamic properties of models. A strong grasp of these concepts is essential for creating realistic and predictable robot behaviors in your virtual environments.","sidebar":"tutorialSidebar"},"module2-gazebo/chapter3":{"id":"module2-gazebo/chapter3","title":"URDF and SDF Formats: Robot and Environment Descriptions","description":"To effectively simulate robots in Gazebo, you need a way to describe their physical properties, kinematic and dynamic structures, and sensor configurations. Similarly, the simulation environment itself needs to be defined, including static objects, terrains, and lighting. This is where the Unified Robot Description Format (URDF) and Simulation Description Format (SDF) come into play.","sidebar":"tutorialSidebar"},"module2-gazebo/chapter4":{"id":"module2-gazebo/chapter4","title":"Unity Visualization: Complementing Gazebo Simulations","description":"While Gazebo is the primary simulation environment for physics-based robotics, powerful 3D visualization tools like Unity can play a complementary role, especially for enhanced human-robot interaction, advanced rendering, or user interface development. This chapter briefly explores how Unity can be used alongside Gazebo, focusing on scenarios where its advanced graphics capabilities might be beneficial for a humanoid robotics textbook.","sidebar":"tutorialSidebar"},"module2-gazebo/chapter5":{"id":"module2-gazebo/chapter5","title":"Sensor Simulation: LiDAR, Depth Camera, IMU","description":"Robots perceive their environment through a variety of sensors. For effective simulation, these sensors must be accurately modeled in the virtual world. Gazebo provides extensive capabilities for simulating common robotic sensors, allowing you to test perception algorithms and validate control strategies without requiring physical hardware. This chapter will guide you through integrating essential sensor models \u2013 LiDAR, depth camera, and IMU \u2013 into your humanoid robot\'s URDF description and configuring them for use within Gazebo.","sidebar":"tutorialSidebar"},"module3-isaac/chapter1":{"id":"module3-isaac/chapter1","title":"Isaac Sim Introduction: Overview and Capabilities","description":"NVIDIA Isaac Sim is a powerful, extensible robotics simulation platform built on NVIDIA Omniverse. It provides a highly realistic, physically accurate, and photorealistic virtual environment for developing, testing, and training AI-powered robots. Unlike traditional simulators like Gazebo, Isaac Sim leverages the full power of NVIDIA GPUs for advanced rendering, sensor simulation, and accelerated AI model training, making it particularly well-suited for applications involving deep learning, reinforcement learning, and sim-to-real transfer.","sidebar":"tutorialSidebar"},"module3-isaac/chapter2":{"id":"module3-isaac/chapter2","title":"AI-powered Perception: Object Detection, Depth Perception","description":"Perception is a cornerstone of intelligent robotics, allowing robots to understand their environment, identify objects, and navigate safely. For humanoid robots, robust perception systems are even more critical due to their complex interactions with the world and the need for sophisticated manipulation and locomotion. NVIDIA Isaac Sim, with its high-fidelity sensor simulation and deep integration with AI frameworks, provides an excellent platform for developing and testing AI-powered perception modules.","sidebar":"tutorialSidebar"},"module3-isaac/chapter3":{"id":"module3-isaac/chapter3","title":"Isaac ROS: Hardware-Accelerated SLAM and Navigation","description":"For a humanoid robot to operate autonomously in complex and dynamic environments, it must be able to perceive its surroundings, localize itself within a map, and navigate effectively. This is the domain of Simultaneous Localization and Mapping (SLAM) and navigation. NVIDIA Isaac ROS provides a collection of hardware-accelerated ROS 2 packages that leverage NVIDIA GPUs to significantly boost the performance of these crucial robotics algorithms, making real-time SLAM and navigation feasible for demanding applications like humanoid locomotion.","sidebar":"tutorialSidebar"},"module3-isaac/chapter4":{"id":"module3-isaac/chapter4","title":"Reinforcement Learning: Training Humanoid Behavior","description":"Reinforcement Learning (RL) has emerged as a powerful paradigm for training complex robot behaviors, particularly for highly dynamic and dexterous tasks that are difficult to program manually. For humanoid robots, RL offers a promising path to developing adaptive locomotion, robust balancing, and intelligent manipulation skills. NVIDIA Isaac Sim provides a specialized platform with tools like Isaac Gym, which accelerates RL training by enabling massive parallelization of simulations.","sidebar":"tutorialSidebar"},"module3-isaac/chapter5":{"id":"module3-isaac/chapter5","title":"Sim-to-Real Transfer: Deploying Models to Real Robots","description":"The ultimate goal of developing robot intelligence in simulation is to deploy it to a physical robot, allowing the robot to perform tasks in the real world. This process, known as Sim-to-Real Transfer, is a crucial yet challenging step in robotics. It involves bridging the gap between the idealized virtual environment and the complexities of the physical world, where unmodeled physics, sensor noise, actuator limitations, and environmental variations can significantly degrade a model\'s performance.","sidebar":"tutorialSidebar"},"module4-vla/chapter1":{"id":"module4-vla/chapter1","title":"Voice-to-Action with Whisper: Capturing and Interpreting Commands","description":"Natural Language Processing (NLP) plays a pivotal role in enabling more intuitive and human-like interactions with robots. For humanoid robots, the ability to understand and act upon spoken commands is a significant step towards seamless collaboration. This chapter explores how to integrate voice commands into a robotic system using advanced speech-to-text models, specifically focusing on OpenAI\'s Whisper, and translating these into actionable instructions for a ROS 2-controlled humanoid.","sidebar":"tutorialSidebar"},"module4-vla/chapter2":{"id":"module4-vla/chapter2","title":"Cognitive Planning: LLM Planning of Sequences","description":"Beyond simple, direct voice commands, the true power of Vision-Language-Action (VLA) models for humanoid robotics emerges when Large Language Models (LLMs) are used for high-level cognitive planning. This involves translating complex, abstract natural language instructions into a sequence of executable robot actions. Instead of explicitly programming every step, an LLM can infer the user\'s intent, break it down into sub-goals, and generate a logical plan for the robot to follow.","sidebar":"tutorialSidebar"},"module4-vla/chapter3":{"id":"module4-vla/chapter3","title":"ROS 2 Action Sequencing: Executing Planned Actions","description":"In the previous chapter, we explored how Large Language Models (LLMs) can generate high-level plans\u2014sequences of abstract actions\u2014from natural language instructions. However, these plans need to be translated into concrete, executable commands for the robot. This is where ROS 2 Action Sequencing comes into play: it\'s the bridge between the cognitive planning layer (LLM) and the low-level robot control.","sidebar":"tutorialSidebar"},"module4-vla/chapter4":{"id":"module4-vla/chapter4","title":"Multi-modal Interaction: Speech, Gesture, Vision Integration","description":"Human-robot interaction becomes significantly more natural and effective when robots can perceive and interpret information from multiple modalities, mirroring how humans communicate. For humanoid robots, integrating speech, gesture, and vision allows for richer understanding of human intent and more intuitive responses. This chapter explores the principles of multi-modal interaction and discusses strategies for combining sensory inputs to create a more sophisticated communication channel between humans and humanoids.","sidebar":"tutorialSidebar"},"module4-vla/chapter5":{"id":"module4-vla/chapter5","title":"Capstone Integration: Autonomous Humanoid Demonstration","description":"Throughout this textbook, we have systematically built the foundational knowledge and practical skills necessary for developing intelligent humanoid robots. We started with the robotic nervous system (ROS 2), moved to creating digital twins in simulation (Gazebo and Isaac Sim), delved into the AI-robot brain (Isaac ROS and Reinforcement Learning), and finally integrated vision, language, and action (VLA) models for cognitive planning and multi-modal interaction. This capstone chapter brings all these concepts together into a comprehensive autonomous humanoid demonstration.","sidebar":"tutorialSidebar"},"writing-guidelines":{"id":"writing-guidelines","title":"Writing Guidelines for Physical AI & Humanoid Robotics Textbook","description":"These guidelines are extracted from the project\'s constitution.md and must be followed for all content in the \\"Physical AI & Humanoid Robotics Textbook\\"."}}}}')}}]);
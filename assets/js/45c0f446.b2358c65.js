"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[5075],{1845:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module2-gazebo/chapter5","title":"Sensor Simulation: LiDAR, Depth Camera, IMU","description":"Robots perceive their environment through a variety of sensors. For effective simulation, these sensors must be accurately modeled in the virtual world. Gazebo provides extensive capabilities for simulating common robotic sensors, allowing you to test perception algorithms and validate control strategies without requiring physical hardware. This chapter will guide you through integrating essential sensor models \u2013 LiDAR, depth camera, and IMU \u2013 into your humanoid robot\'s URDF description and configuring them for use within Gazebo.","source":"@site/docs/module2-gazebo/chapter5.md","sourceDirName":"module2-gazebo","slug":"/module2-gazebo/chapter5","permalink":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter5","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module2-gazebo/chapter5.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Unity Visualization: Complementing Gazebo Simulations","permalink":"/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter4"},"next":{"title":"Isaac Sim Introduction: Overview and Capabilities","permalink":"/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter1"}}');var i=r(4848),o=r(8453);const s={},t="Sensor Simulation: LiDAR, Depth Camera, IMU",l={},d=[{value:"Understanding Sensor Models in URDF/SDF",id:"understanding-sensor-models-in-urdfsdf",level:2},{value:"Common Sensor Types",id:"common-sensor-types",level:3},{value:"Integrating Sensors into the Humanoid URDF (Xacro)",id:"integrating-sensors-into-the-humanoid-urdf-xacro",level:2},{value:"Example: Adding Sensors to <code>generic_humanoid.urdf.xacro</code>",id:"example-adding-sensors-to-generic_humanoidurdfxacro",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sensor-simulation-lidar-depth-camera-imu",children:"Sensor Simulation: LiDAR, Depth Camera, IMU"})}),"\n",(0,i.jsx)(n.p,{children:"Robots perceive their environment through a variety of sensors. For effective simulation, these sensors must be accurately modeled in the virtual world. Gazebo provides extensive capabilities for simulating common robotic sensors, allowing you to test perception algorithms and validate control strategies without requiring physical hardware. This chapter will guide you through integrating essential sensor models \u2013 LiDAR, depth camera, and IMU \u2013 into your humanoid robot's URDF description and configuring them for use within Gazebo."}),"\n",(0,i.jsx)(n.h2,{id:"understanding-sensor-models-in-urdfsdf",children:"Understanding Sensor Models in URDF/SDF"}),"\n",(0,i.jsxs)(n.p,{children:["While basic sensor definitions can sometimes be part of URDF, for full fidelity and Gazebo-specific parameters (like noise, update rates, and visualization), sensors are primarily defined using Gazebo plugins within an SDF context. When working with URDF, Gazebo translates URDF ",(0,i.jsx)(n.code,{children:"<gazebo>"})," tags (which contain SDF snippets) into its internal SDF representation."]}),"\n",(0,i.jsx)(n.h3,{id:"common-sensor-types",children:"Common Sensor Types"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging)"}),": Used for measuring distances to objects and creating 2D or 3D maps of the environment. In Gazebo, LiDARs are often simulated as ",(0,i.jsx)(n.code,{children:"ray"})," sensors."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Camera"}),": Provides both color (RGB) images and depth information, crucial for 3D perception, object detection, and navigation. Simulated as ",(0,i.jsx)(n.code,{children:"camera"})," sensors with depth capabilities."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit)"}),": Measures orientation, angular velocity, and linear acceleration. Essential for robot localization, balancing, and control. Simulated as ",(0,i.jsx)(n.code,{children:"imu"})," sensors."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"integrating-sensors-into-the-humanoid-urdf-xacro",children:"Integrating Sensors into the Humanoid URDF (Xacro)"}),"\n",(0,i.jsxs)(n.p,{children:["We will enhance our ",(0,i.jsx)(n.code,{children:"generic_humanoid.urdf.xacro"})," model to include these sensors. This involves adding new links for the sensors, defining their joints to the robot's body, and incorporating Gazebo-specific sensor definitions using ",(0,i.jsx)(n.code,{children:"<gazebo>"})," tags within the Xacro file."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key considerations when adding sensors:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Placement"}),": Sensible placement on the robot model (e.g., camera on the head, LiDAR on the torso)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Link"}),": Each sensor typically has its own ",(0,i.jsx)(n.code,{children:"<link>"})," in the URDF, which makes its pose and parent-child relationship explicit."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gazebo Plugins"}),": The core of sensor simulation in Gazebo is done via plugins. These are specified within ",(0,i.jsx)(n.code,{children:"<gazebo>"})," tags."]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"example-adding-sensors-to-generic_humanoidurdfxacro",children:["Example: Adding Sensors to ",(0,i.jsx)(n.code,{children:"generic_humanoid.urdf.xacro"})]}),"\n",(0,i.jsx)(n.p,{children:"We'll add a head-mounted depth camera, a torso-mounted 2D LiDAR, and an IMU within the torso."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Depth Camera (on Head Link)"})}),"\n",(0,i.jsxs)(n.p,{children:["Add a new ",(0,i.jsx)(n.code,{children:"camera_link"})," and ",(0,i.jsx)(n.code,{children:"camera_joint"})," to the Xacro file, attached to the ",(0,i.jsx)(n.code,{children:"head_link"}),". Then, define a Gazebo camera sensor plugin."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'  \x3c!-- Depth Camera Joint and Link --\x3e\r\n  <joint name="camera_joint" type="fixed">\r\n    <parent link="head_link"/>\r\n    <child link="camera_link"/>\r\n    <origin xyz="0.05 0 0" rpy="0 0 0"/>\r\n  </joint>\r\n  <link name="camera_link">\r\n    <visual>\r\n      <geometry>\r\n        <box size="0.02 0.05 0.05"/>\r\n      </geometry>\r\n      <material name="red">\r\n        <color rgba="1 0 0 1"/>\r\n      </material>\r\n    </visual>\r\n  </link>\r\n\r\n  \x3c!-- Gazebo Camera Sensor --\x3e\r\n  <gazebo reference="camera_link">\r\n    <sensor name="depth_camera" type="depth">\r\n      <update_rate>30.0</update_rate>\r\n      <camera name="head_camera">\r\n        <horizontal_fov>1.047</horizontal_fov>\r\n        <image>\r\n          <width>640</width>\r\n          <height>480</height>\r\n          <format>R8G8B8</format>\r\n        </image>\r\n        <clip>\r\n          <near>0.1</near>\r\n          <far>10</far>\r\n        </clip>\r\n        <noise>\r\n          <type>gaussian</type>\r\n          <mean>0.0</mean>\r\n          <stddev>0.007</stddev>\r\n        </noise>\r\n      </camera>\r\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\r\n        <ros>\r\n          <namespace>camera</namespace>\r\n          <argument>--ros-args -r image:=image_raw -r depth_image:=depth/image_raw</argument>\r\n          <argument>--ros-args -r camera_info:=camera_info -r depth/camera_info:=depth/camera_info</argument>\r\n        </ros>\r\n        <camera_name>depth_camera</camera_name>\r\n        <frame_name>camera_depth_frame</frame_name>\r\n        <hack_baseline>0.07</hack_baseline>\r\n        <min_depth>0.1</min_depth>\r\n        <max_depth>10.0</max_depth>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. 2D LiDAR (on Torso Link)"})}),"\n",(0,i.jsxs)(n.p,{children:["Add a ",(0,i.jsx)(n.code,{children:"laser_link"})," and ",(0,i.jsx)(n.code,{children:"laser_joint"})," to the ",(0,i.jsx)(n.code,{children:"torso_link"}),". Define a Gazebo ray sensor plugin."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'  \x3c!-- LiDAR Joint and Link --\x3e\r\n  <joint name="laser_joint" type="fixed">\r\n    <parent link="torso_link"/>\r\n    <child link="laser_link"/>\r\n    <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\r\n  </joint>\r\n  <link name="laser_link">\r\n    <visual>\r\n      <geometry>\r\n        <cylinder radius="0.03" length="0.04"/>\r\n      </geometry>\r\n      <material name="green">\r\n        <color rgba="0 1 0 1"/>\r\n      </material>\r\n    </visual>\r\n  </link>\r\n\r\n  \x3c!-- Gazebo LiDAR Sensor --\x3e\r\n  <gazebo reference="laser_link">\r\n    <sensor name="lidar" type="ray">\r\n      <pose>0 0 0 0 0 0</pose>\r\n      <visualize>true</visualize>\r\n      <update_rate>10.0</update_rate>\r\n      <ray>\r\n        <scan>\r\n          <horizontal>\r\n            <samples>640</samples>\r\n            <resolution>1</resolution>\r\n            <min_angle>-1.5708</min_angle>\r\n            <max_angle>1.5708</max_angle>\r\n          </horizontal>\r\n        </scan>\r\n        <range>\r\n          <min>0.1</min>\r\n          <max>10.0</max>\r\n          <resolution>0.01</resolution>\r\n        </range>\r\n        <noise>\r\n          <type>gaussian</type>\r\n          <mean>0.0</mean>\r\n          <stddev>0.01</stddev>\r\n        </noise>\r\n      </ray>\r\n      <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">\r\n        <ros>\r\n          <argument>--ros-args -r scan:=scan</argument>\r\n          <namespace>lidar</namespace>\r\n        </ros>\r\n        <output_type>sensor_msgs/LaserScan</output_type>\r\n        <frame_name>laser_frame</frame_name>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. IMU (within Torso Link)"})}),"\n",(0,i.jsx)(n.p,{children:"The IMU is usually embedded directly within a link (e.g., the base link or torso link) and doesn't always require a separate visual link."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'  \x3c!-- Gazebo IMU Sensor (reference torso_link) --\x3e\r\n  <gazebo reference="torso_link">\r\n    <sensor name="imu_sensor" type="imu">\r\n      <always_on>true</always_on>\r\n      <update_rate>100.0</update_rate>\r\n      <visualize>false</visualize>\r\n      <topic>imu</topic>\r\n      <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\r\n        <ros>\r\n          <namespace>/imu</namespace>\r\n          <argument>--ros-args -r imu:=data</argument>\r\n        </ros>\r\n        <frame_name>torso_link</frame_name>\r\n        <initial_orientation_as_reference>false</initial_orientation_as_reference>\r\n        <gyroscope>\r\n          <x>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.0002</stddev>\r\n            </noise>\r\n          </x>\r\n          <y>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.0002</stddev>\r\n            </noise>\r\n          </y>\r\n          <z>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.0002</stddev>\r\n            </noise>\r\n          </z>\r\n        </gyroscope>\r\n        <accelerometer>\r\n          <x>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.017</stddev>\r\n            </noise>\r\n          </x>\r\n          <y>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.017</stddev>\r\n            </noise>\r\n          </y>\r\n          <z>\r\n            <noise type="gaussian">\r\n              <mean>0.0</mean>\r\n              <stddev>0.017</stddev>\r\n            </noise>\r\n          </z>\r\n        </accelerometer>\r\n      </plugin>\r\n    </sensor>\r\n  </gazebo>\r\n\r\n</robot>\n'})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>t});var a=r(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);
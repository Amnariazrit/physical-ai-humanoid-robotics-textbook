"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[1923],{5080:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>i,default:()=>g,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module4-vla/chapter3","title":"ROS 2 Action Sequencing: Executing Planned Actions","description":"In the previous chapter, we explored how Large Language Models (LLMs) can generate high-level plans\u2014sequences of abstract actions\u2014from natural language instructions. However, these plans need to be translated into concrete, executable commands for the robot. This is where ROS 2 Action Sequencing comes into play: it\'s the bridge between the cognitive planning layer (LLM) and the low-level robot control.","source":"@site/docs/module4-vla/chapter3.md","sourceDirName":"module4-vla","slug":"/module4-vla/chapter3","permalink":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter3","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/chapter3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning: LLM Planning of Sequences","permalink":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2"},"next":{"title":"Multi-modal Interaction: Speech, Gesture, Vision Integration","permalink":"/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter4"}}');var t=o(4848),l=o(8453);const s={},i="ROS 2 Action Sequencing: Executing Planned Actions",a={},c=[{value:"Architecture for Action Sequencing",id:"architecture-for-action-sequencing",level:2},{value:"Key ROS 2 Components for Sequencing:",id:"key-ros-2-components-for-sequencing",level:3},{value:"Designing Robot Skills (Actions)",id:"designing-robot-skills-actions",level:2},{value:"Python Example: Executing a Simple LLM-Generated Plan",id:"python-example-executing-a-simple-llm-generated-plan",level:2},{value:"Conceptual Action Sequencing Node:",id:"conceptual-action-sequencing-node",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"ros-2-action-sequencing-executing-planned-actions",children:"ROS 2 Action Sequencing: Executing Planned Actions"})}),"\n",(0,t.jsx)(n.p,{children:"In the previous chapter, we explored how Large Language Models (LLMs) can generate high-level plans\u2014sequences of abstract actions\u2014from natural language instructions. However, these plans need to be translated into concrete, executable commands for the robot. This is where ROS 2 Action Sequencing comes into play: it's the bridge between the cognitive planning layer (LLM) and the low-level robot control."}),"\n",(0,t.jsx)(n.p,{children:"This chapter will focus on designing a ROS 2 system that can parse a sequence of actions generated by an LLM and execute them on a humanoid robot using ROS 2 Actions and other communication mechanisms. We'll discuss how to create a robust action sequencing framework that can handle execution, monitor progress, and report status."}),"\n",(0,t.jsx)(n.h2,{id:"architecture-for-action-sequencing",children:"Architecture for Action Sequencing"}),"\n",(0,t.jsx)(n.p,{children:"Executing an LLM-generated plan typically involves a dedicated ROS 2 node or a set of nodes responsible for:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Plan Reception"}),": Receiving the sequence of high-level actions (e.g., a list of strings or a custom message type) from the LLM planning module."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Dispatcher"}),": Translating each abstract action into a specific ROS 2 Action Goal, Service Request, or Topic Message."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Monitoring"}),": Sending the ROS 2 commands to the appropriate robot controllers (action servers, service servers, topic subscribers) and monitoring their execution status."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feedback and Reporting"}),": Providing feedback to the LLM (or a human operator) about the progress and outcome of each action in the sequence."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling and Replanning"}),": Detecting execution failures and, if necessary, triggering the LLM to generate a revised plan."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-ros-2-components-for-sequencing",children:"Key ROS 2 Components for Sequencing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Actions"}),": Ideal for executing individual high-level skills (e.g., ",(0,t.jsx)(n.code,{children:"move_to_location"}),", ",(0,t.jsx)(n.code,{children:"grasp_object"}),", ",(0,t.jsx)(n.code,{children:"open_door"}),"). Each action provides a goal, feedback, and result, which is crucial for monitoring plan progress."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Services"}),": Can be used for instantaneous, blocking operations that are part of a sequence (e.g., ",(0,t.jsx)(n.code,{children:"set_gripper_force"}),", ",(0,t.jsx)(n.code,{children:"get_object_info"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Topics"}),": For continuous feedback from robot sensors or state updates."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"designing-robot-skills-actions",children:"Designing Robot Skills (Actions)"}),"\n",(0,t.jsx)(n.p,{children:'For an LLM to generate an executable plan, the robot needs a well-defined "API" of skills. These skills should map to ROS 2 Actions or Services.'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Robot Skills as ROS 2 Actions"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"MoveToLocation.action"})}),":\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Goal: ",(0,t.jsx)(n.code,{children:"string location_name"}),' (e.g., "kitchen", "fridge")']}),"\n",(0,t.jsxs)(n.li,{children:["Result: ",(0,t.jsx)(n.code,{children:"bool success"})]}),"\n",(0,t.jsxs)(n.li,{children:["Feedback: ",(0,t.jsx)(n.code,{children:"string current_location"}),", ",(0,t.jsx)(n.code,{children:"float distance_remaining"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"GraspObject.action"})}),":\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Goal: ",(0,t.jsx)(n.code,{children:"string object_name"}),' (e.g., "bottle", "cup")']}),"\n",(0,t.jsxs)(n.li,{children:["Result: ",(0,t.jsx)(n.code,{children:"bool success"}),", ",(0,t.jsx)(n.code,{children:"string grasped_object"})]}),"\n",(0,t.jsxs)(n.li,{children:["Feedback: ",(0,t.jsx)(n.code,{children:"string status_message"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"OpenDoor.action"})}),":\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Goal: ",(0,t.jsx)(n.code,{children:"string door_name"}),' (e.g., "fridge_door")']}),"\n",(0,t.jsxs)(n.li,{children:["Result: ",(0,t.jsx)(n.code,{children:"bool success"})]}),"\n",(0,t.jsxs)(n.li,{children:["Feedback: ",(0,t.jsx)(n.code,{children:"string status_message"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"python-example-executing-a-simple-llm-generated-plan",children:"Python Example: Executing a Simple LLM-Generated Plan"}),"\n",(0,t.jsx)(n.p,{children:"Let's assume an LLM has provided the following plan as a list of strings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'plan = [\r\n    "move_to(fridge)",\r\n    "open(fridge_door)",\r\n    "grasp(bottle)",\r\n    "close(fridge_door)",\r\n    "move_to(table)"\r\n]\n'})}),"\n",(0,t.jsx)(n.p,{children:"We need a ROS 2 Python node to execute this."}),"\n",(0,t.jsx)(n.h3,{id:"conceptual-action-sequencing-node",children:"Conceptual Action Sequencing Node:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom rclpy.callback_groups import ReentrantCallbackGroup\r\nfrom rclpy.executors import MultiThreadedExecutor\r\n\r\n# Import custom action messages (e.g., from a package like 'robot_skill_interfaces')\r\nfrom robot_skill_interfaces.action import MoveToLocation, GraspObject, OpenDoor, CloseDoor\r\n\r\nclass ActionSequencer(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__('action_sequencer')\r\n        self.declare_parameter('llm_plan', [])\r\n        self.llm_plan = []\r\n        self.current_action_index = 0\r\n\r\n        self.callback_group = ReentrantCallbackGroup()\r\n\r\n        # Action Clients for each skill\r\n        self._move_client = ActionClient(self, MoveToLocation, 'move_to_location', callback_group=self.callback_group)\r\n        self._grasp_client = ActionClient(self, GraspObject, 'grasp_object', callback_group=self.callback_group)\r\n        self._open_client = ActionClient(self, OpenDoor, 'open_door', callback_group=self.callback_group)\r\n        self._close_client = ActionClient(self, CloseDoor, 'close_door', callback_group=self.callback_group)\r\n\r\n        # Map action names to clients\r\n        self.action_clients = {\r\n            'move_to': self._move_client,\r\n            'grasp': self._grasp_client,\r\n            'open': self._open_client,\r\n            'close': self._close_client,\r\n        }\r\n\r\n        self.get_logger().info(\"Action Sequencer Node initialized.\")\r\n\r\n        self.plan_executor_timer = self.create_timer(1.0, self.execute_next_action_in_plan)\r\n        self.plan_executor_timer.cancel() # Start paused\r\n\r\n    def start_plan_execution(self, plan_str_list):\r\n        self.llm_plan = plan_str_list\r\n        self.current_action_index = 0\r\n        self.get_logger().info(f\"Starting plan execution: {self.llm_plan}\")\r\n        self.plan_executor_timer.reset()\r\n\r\n    def execute_next_action_in_plan(self):\r\n        if self.current_action_index >= len(self.llm_plan):\r\n            self.get_logger().info(\"Plan execution complete.\")\r\n            self.executing_action = False\r\n            return\r\n\r\n        action_str = self.llm_plan[self.current_action_index]\r\n        self.get_logger().info(f\"Executing action: {action_str}\")\r\n        self.executing_action = True\r\n\r\n        # Parse action string (simplified for example)\r\n        # Example: \"move_to(fridge)\" -> action_name=\"move_to\", param=\"fridge\"\r\n        action_name_full = action_str.split('(')[0]\r\n        params_str = action_str.split('(')[1].strip(')')\r\n\r\n        action_name = action_name_full.split('_', 1)[0] # e.g., 'move', 'grasp'\r\n\r\n        if action_name_full in self.action_clients: # Direct match for skill name\r\n            client = self.action_clients[action_name_full]\r\n            \r\n            # Wait for action server\r\n            if not client.wait_for_server(timeout_sec=5.0):\r\n                self.get_logger().error(f\"Action server for {action_name_full} not available. Aborting plan.\")\r\n                self.executing_action = False\r\n                return\r\n\r\n            goal_msg = None\r\n            if action_name_full == 'move_to':\r\n                goal_msg = MoveToLocation.Goal()\r\n                goal_msg.location_name = params_str\r\n            elif action_name_full == 'grasp':\r\n                goal_msg = GraspObject.Goal()\r\n                parts = params_str.split(',', 1) # Split object name and position\r\n                goal_msg.object_name = parts[0].strip()\r\n                if len(parts) > 1:\r\n                    goal_msg.object_position = parts[1].strip() # Example: \"x=0.5,y=0.1,z=0.0\"\r\n                else:\r\n                    goal_msg.object_position = \"\" # No position given\r\n            elif action_name_full == 'open':\r\n                goal_msg = OpenDoor.Goal()\r\n                goal_msg.door_name = params_str\r\n            elif action_name_full == 'close':\r\n                goal_msg = CloseDoor.Goal()\r\n                goal_msg.door_name = params_str\r\n            \r\n            if goal_msg:\r\n                self._send_goal_future = client.send_goal_async(goal_msg)\r\n                self._send_goal_future.add_done_callback(self.goal_response_callback)\r\n            else:\r\n                self.get_logger().error(f\"Failed to create goal message for action: {action_name_full}. Aborting.\")\r\n                self.executing_action = False\r\n        else:\r\n            self.get_logger().error(f\"No action client found for skill: {action_name_full}. Aborting plan.\")\r\n            self.executing_action = False\r\n\r\n    def goal_response_callback(self, future):\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().error(f'Goal rejected by server: {goal_handle.goal_id}')\r\n            self.executing_action = False\r\n            return\r\n\r\n        self.get_logger().info(f'Goal accepted by server: {goal_handle.goal_id}')\r\n        self._get_result_future = goal_handle.get_result_async()\r\n        self._get_result_future.add_done_callback(self.get_result_callback)\r\n\r\n    def get_result_callback(self, future):\r\n        result = future.result().result\r\n        if result.success:\r\n            self.get_logger().info(f\"Action completed successfully.\")\r\n            self.current_action_index += 1\r\n            # Wait a moment before next action for simulation realism\r\n            time.sleep(0.5)\r\n            self.execute_next_action() # Proceed to the next action in the plan\r\n        else:\r\n            self.get_logger().error(f\"Action failed. Aborting plan.\")\r\n            self.executing_action = False\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    executor = MultiThreadedExecutor()\r\n    action_sequencer_node = ActionSequencerNode()\r\n    executor.add_node(action_sequencer_node)\r\n\r\n    try:\r\n        executor.spin()\r\n    except KeyboardInterrupt:\r\n        action_sequencer_node.get_logger().info(\"Action sequencer stopped.\")\r\n    finally:\r\n        executor.shutdown()\r\n        action_sequencer_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})})]})}function g(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>i});var r=o(6540);const t={},l=r.createContext(t);function s(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);
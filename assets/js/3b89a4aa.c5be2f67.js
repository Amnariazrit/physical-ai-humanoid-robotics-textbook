"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[6980],{2590:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/blog2-399b2e250379a8d977f21c3da7a03fd1.png"},3132:e=>{e.exports=JSON.parse('{"permalink":"/physical-ai-humanoid-robotics-textbook/blog/cognitive-planning-llms","source":"@site/blog/2025-12-05-cognitive-planning-llms.mdx","title":"Cognitive Planning in Humanoids: When LLMs Meet Robotic Action","description":"Cognitive Planning Hero Image","date":"2025-12-05T00:00:00.000Z","tags":[{"inline":true,"label":"LLMs","permalink":"/physical-ai-humanoid-robotics-textbook/blog/tags/ll-ms"},{"inline":true,"label":"cognitive_planning","permalink":"/physical-ai-humanoid-robotics-textbook/blog/tags/cognitive-planning"},{"inline":true,"label":"AI","permalink":"/physical-ai-humanoid-robotics-textbook/blog/tags/ai"},{"inline":true,"label":"robotics","permalink":"/physical-ai-humanoid-robotics-textbook/blog/tags/robotics"},{"inline":true,"label":"VLA","permalink":"/physical-ai-humanoid-robotics-textbook/blog/tags/vla"},{"inline":true,"label":"humanoids","permalink":"/physical-ai-humanoid-robotics-textbook/blog/tags/humanoids"}],"readingTime":3.39,"hasTruncateMarker":false,"authors":[{"name":"Gemini Agent","title":"AI Assistant","url":"https://github.com/google-gemini","key":"gemini_agent","page":null}],"frontMatter":{"slug":"cognitive-planning-llms","title":"Cognitive Planning in Humanoids: When LLMs Meet Robotic Action","authors":["gemini_agent"],"tags":["LLMs","cognitive_planning","AI","robotics","VLA","humanoids"]},"unlisted":false,"nextItem":{"title":"The Rise of Humanoid Robotics: A New Era of AI in Motion","permalink":"/physical-ai-humanoid-robotics-textbook/blog/humanoid-robotics-era"}}')},8047:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});var t=i(3132),o=i(4848),a=i(8453);const s={slug:"cognitive-planning-llms",title:"Cognitive Planning in Humanoids: When LLMs Meet Robotic Action",authors:["gemini_agent"],tags:["LLMs","cognitive_planning","AI","robotics","VLA","humanoids"]},r=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Beyond Hard-Coded Tasks: The LLM Revolution in Robotics",id:"beyond-hard-coded-tasks-the-llm-revolution-in-robotics",level:2},{value:"The Brains Behind the Brawn: How LLMs Plan",id:"the-brains-behind-the-brawn-how-llms-plan",level:2},{value:"The Architecture: Bridging Language and Action",id:"the-architecture-bridging-language-and-action",level:2},{value:"Challenges and the Road Ahead",id:"challenges-and-the-road-ahead",level:2}];function d(e){const n={code:"code",h2:"h2",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Cognitive Planning Hero Image",src:i(2590).A+"",width:"784",height:"1168"})}),"\n",(0,o.jsx)(n.h2,{id:"beyond-hard-coded-tasks-the-llm-revolution-in-robotics",children:"Beyond Hard-Coded Tasks: The LLM Revolution in Robotics"}),"\n",(0,o.jsx)(n.p,{children:"For decades, teaching robots to perform complex tasks has involved meticulous programming, often requiring engineers to define every step, every condition, and every exception. This approach is brittle, difficult to scale, and struggles with the inherent ambiguity of real-world environments and human instructions. Enter Large Language Models (LLMs) \u2013 the same technology powering conversational AI \u2013 now poised to revolutionize how humanoid robots plan and execute actions."}),"\n",(0,o.jsx)(n.p,{children:"The integration of LLMs with robotic systems, particularly for cognitive planning, marks a significant leap towards truly autonomous and adaptable humanoids. This paradigm shift moves robots beyond rigid, pre-programmed behaviors to a state where they can understand high-level, abstract goals and dynamically generate action sequences to achieve them."}),"\n",(0,o.jsx)(n.h2,{id:"the-brains-behind-the-brawn-how-llms-plan",children:"The Brains Behind the Brawn: How LLMs Plan"}),"\n",(0,o.jsx)(n.p,{children:"At its core, cognitive planning with LLMs involves leveraging their ability to reason, understand context, and synthesize information from vast datasets. For a humanoid robot, an LLM acts as a high-level brain, translating fuzzy human intent into a concrete, executable plan."}),"\n",(0,o.jsx)(n.p,{children:'Consider a human command like, "Robot, please get me a bottle from the fridge." A traditional robot would struggle unless explicitly programmed for this exact scenario. An LLM, however, can:'}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decompose the Task"}),': Break down "get bottle from fridge" into sub-goals: "go to fridge," "open fridge door," "locate bottle," "grasp bottle," "close fridge door," "bring bottle to human."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Access Knowledge"}),': Understand what a "fridge" is, that it has a "door" that needs to be "opened" to access contents, and what "grasping" entails.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reason about State"}),': If the fridge door is already open, the LLM can skip the "open fridge door" step. If the robot\'s current location is not near the fridge, it can infer a "move to fridge" action.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Generate Action Sequences"}),": Output a sequence of primitive robot skills (e.g., ",(0,o.jsx)(n.code,{children:"move_to(location)"}),", ",(0,o.jsx)(n.code,{children:"grasp(object)"}),", ",(0,o.jsx)(n.code,{children:"open(door)"}),") that can be executed by the robot's lower-level control systems."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This capability allows humanoids to respond to novel commands and adapt to unforeseen circumstances without constant human re-programming."}),"\n",(0,o.jsx)(n.h2,{id:"the-architecture-bridging-language-and-action",children:"The Architecture: Bridging Language and Action"}),"\n",(0,o.jsx)(n.p,{children:"A typical architecture for LLM-based cognitive planning in humanoids involves:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception Systems"}),": Cameras, LiDARs, IMUs (often accelerated by NVIDIA Isaac ROS) provide real-time information about the environment, objects, and the robot's own state."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech-to-Text (e.g., Whisper)"}),": Converts human voice commands into text that the LLM can process."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LLM Interface"}),": A ROS 2 node acts as an interface, feeding environmental context and human instructions to the LLM, and receiving the generated action plan."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Sequencer"}),": A critical component that takes the LLM's high-level action plan and translates it into specific ROS 2 Action Goals, Service Calls, or Topic Commands for the robot's low-level controllers."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robot Actuators & Controllers"}),": The physical hardware that executes the commands, often managed by ",(0,o.jsx)(n.code,{children:"ros2_control"})," in simulation (e.g., Gazebo, Isaac Sim) or on real hardware."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This modularity, enabled by ROS 2, allows each component to evolve independently while contributing to the overall intelligence of the humanoid."}),"\n",(0,o.jsx)(n.h2,{id:"challenges-and-the-road-ahead",children:"Challenges and the Road Ahead"}),"\n",(0,o.jsx)(n.p,{children:"While promising, LLM-based cognitive planning faces several challenges:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grounding"}),': Ensuring the LLM\'s abstract knowledge is accurately "grounded" in the physical reality of the robot and its environment.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Hallucinations"}),": LLMs can sometimes generate plausible but physically impossible or unsafe actions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety & Robustness"}),": Implementing safeguards to prevent the robot from executing dangerous plans or failing gracefully when plans encounter unexpected real-world conditions."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Performance"}),": Optimizing LLM inference and planning cycles for the low-latency demands of robotic control."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ethical Considerations"}),": As robots become more autonomous, the ethical implications of their decision-making, even if derived from an LLM, become paramount."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Despite these hurdles, the fusion of LLMs with humanoid robotics represents an exciting frontier. This textbook provides the foundation for understanding and contributing to this future, where intelligent humanoids can learn, adapt, and interact with the world in ways previously confined to imagination."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Image placeholder reminder:"})," Please replace ",(0,o.jsx)(n.code,{children:"/img/humanoid-blog-2-hero.png"})," with a relevant high-quality image."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var t=i(6540);const o={},a=t.createContext(o);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);
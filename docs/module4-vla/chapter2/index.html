<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module4-vla/chapter2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Cognitive Planning: LLM Planning of Sequences | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Cognitive Planning: LLM Planning of Sequences | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="Beyond simple, direct voice commands, the true power of Vision-Language-Action (VLA) models for humanoid robotics emerges when Large Language Models (LLMs) are used for high-level cognitive planning. This involves translating complex, abstract natural language instructions into a sequence of executable robot actions. Instead of explicitly programming every step, an LLM can infer the user&#x27;s intent, break it down into sub-goals, and generate a logical plan for the robot to follow."><meta data-rh="true" property="og:description" content="Beyond simple, direct voice commands, the true power of Vision-Language-Action (VLA) models for humanoid robotics emerges when Large Language Models (LLMs) are used for high-level cognitive planning. This involves translating complex, abstract natural language instructions into a sequence of executable robot actions. Instead of explicitly programming every step, an LLM can infer the user&#x27;s intent, break it down into sub-goals, and generate a logical plan for the robot to follow."><link data-rh="true" rel="icon" href="/physical-ai-humanoid-robotics-textbook/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2"><link data-rh="true" rel="alternate" href="https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2" hreflang="en"><link data-rh="true" rel="alternate" href="https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Module 4: Vision-Language-Action (VLA)","item":"https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1"},{"@type":"ListItem","position":2,"name":"Cognitive Planning: LLM Planning of Sequences","item":"https://Amnariazrit.github.io/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2"}]}</script><link rel="alternate" type="application/rss+xml" href="/physical-ai-humanoid-robotics-textbook/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics Textbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/physical-ai-humanoid-robotics-textbook/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Textbook Atom Feed"><link rel="stylesheet" href="/physical-ai-humanoid-robotics-textbook/assets/css/styles.fc6aef72.css">
<script src="/physical-ai-humanoid-robotics-textbook/assets/js/runtime~main.d7fd4308.js" defer="defer"></script>
<script src="/physical-ai-humanoid-robotics-textbook/assets/js/main.381a09d3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/physical-ai-humanoid-robotics-textbook/"><div class="navbar__logo"><img src="/physical-ai-humanoid-robotics-textbook/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/physical-ai-humanoid-robotics-textbook/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter1">Textbook</a><a class="navbar__item navbar__link" href="/physical-ai-humanoid-robotics-textbook/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/Amnariazrit/physical-ai-humanoid-robotics-textbook" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter1"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/physical-ai-humanoid-robotics-textbook/docs/module2-gazebo/chapter1"><span title="Module 2: The Digital Twin (Gazebo &amp; Unity)" class="categoryLinkLabel_W154">Module 2: The Digital Twin (Gazebo &amp; Unity)</span></a><button aria-label="Expand sidebar category &#x27;Module 2: The Digital Twin (Gazebo &amp; Unity)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/physical-ai-humanoid-robotics-textbook/docs/module3-isaac/chapter1"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaac)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Collapse sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1"><span title="Voice-to-Action with Whisper: Capturing and Interpreting Commands" class="linkLabel_WmDU">Voice-to-Action with Whisper: Capturing and Interpreting Commands</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter2"><span title="Cognitive Planning: LLM Planning of Sequences" class="linkLabel_WmDU">Cognitive Planning: LLM Planning of Sequences</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter3"><span title="ROS 2 Action Sequencing: Executing Planned Actions" class="linkLabel_WmDU">ROS 2 Action Sequencing: Executing Planned Actions</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter4"><span title="Multi-modal Interaction: Speech, Gesture, Vision Integration" class="linkLabel_WmDU">Multi-modal Interaction: Speech, Gesture, Vision Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter5"><span title="Capstone Integration: Autonomous Humanoid Demonstration" class="linkLabel_WmDU">Capstone Integration: Autonomous Humanoid Demonstration</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/physical-ai-humanoid-robotics-textbook/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1"><span>Module 4: Vision-Language-Action (VLA)</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Cognitive Planning: LLM Planning of Sequences</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Cognitive Planning: LLM Planning of Sequences</h1></header>
<p>Beyond simple, direct voice commands, the true power of Vision-Language-Action (VLA) models for humanoid robotics emerges when Large Language Models (LLMs) are used for high-level cognitive planning. This involves translating complex, abstract natural language instructions into a sequence of executable robot actions. Instead of explicitly programming every step, an LLM can infer the user&#x27;s intent, break it down into sub-goals, and generate a logical plan for the robot to follow.</p>
<p>This chapter will delve into the concept of cognitive planning using LLMs for humanoid robots. We&#x27;ll explore how LLMs can act as the &quot;brain&quot; for high-level task decomposition and symbolic reasoning, bridging the gap between human language and robot execution.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-need-for-cognitive-planning">The Need for Cognitive Planning<a href="#the-need-for-cognitive-planning" class="hash-link" aria-label="Direct link to The Need for Cognitive Planning" title="Direct link to The Need for Cognitive Planning" translate="no">​</a></h2>
<p>Traditional robotics often relies on pre-programmed sequences or elaborate state machines for task execution. However, this approach struggles with:</p>
<ul>
<li class=""><strong>Ambiguity</strong>: Human instructions are inherently ambiguous and lack explicit detail for robots.</li>
<li class=""><strong>Novelty</strong>: Robots need to adapt to new situations and tasks not explicitly coded.</li>
<li class=""><strong>Scalability</strong>: Manually programming every possible task permutation is unfeasible.</li>
</ul>
<p>LLMs, with their vast world knowledge and reasoning capabilities, offer a solution by enabling robots to perform:</p>
<ul>
<li class=""><strong>Task Decomposition</strong>: Breaking a high-level goal (&quot;clean the table&quot;) into a series of smaller, manageable steps (&quot;identify objects,&quot; &quot;pick up cup,&quot; &quot;move to sink,&quot; etc.).</li>
<li class=""><strong>Symbolic Reasoning</strong>: Understanding relationships between objects and actions (&quot;a cup can hold water,&quot; &quot;cleaning involves removing items&quot;).</li>
<li class=""><strong>Error Recovery</strong>: Adapting plans if a step fails.</li>
<li class=""><strong>Human-Robot Dialogue</strong>: Engaging in clarifying conversations about the task.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="llms-as-robot-planners">LLMs as Robot Planners<a href="#llms-as-robot-planners" class="hash-link" aria-label="Direct link to LLMs as Robot Planners" title="Direct link to LLMs as Robot Planners" translate="no">​</a></h2>
<p>An LLM can serve as a high-level planner in a hierarchical robotics architecture.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="architecture-overview">Architecture Overview:<a href="#architecture-overview" class="hash-link" aria-label="Direct link to Architecture Overview:" title="Direct link to Architecture Overview:" translate="no">​</a></h3>
<ol>
<li class=""><strong>Human Instruction</strong>: User provides a natural language instruction (e.g., &quot;Please get me a drink from the fridge&quot;).</li>
<li class=""><strong>LLM (Cognitive Planner)</strong>:
<ul>
<li class="">Receives the instruction.</li>
<li class="">Accesses a &quot;robot API&quot; or a knowledge base of available robot skills (e.g., <code>move_to(location)</code>, <code>grasp(object)</code>, <code>open(door)</code>).</li>
<li class="">Generates a sequence of these low-level robot skills/actions to achieve the high-level goal.</li>
<li class="">Can perform reasoning (e.g., &quot;to get a drink, I need to open the fridge, pick up a bottle, close the fridge&quot;).</li>
</ul>
</li>
<li class=""><strong>ROS 2 Action Sequencer (Low-Level Executor)</strong>: Receives the sequence of robot skills from the LLM.
<ul>
<li class="">Translates each skill into specific ROS 2 action goals, service calls, or topic commands.</li>
<li class="">Executes these commands on the robot.</li>
<li class="">Provides feedback on execution status back to the LLM (for replanning if needed).</li>
</ul>
</li>
<li class=""><strong>Robot Hardware/Simulation</strong>: Executes the low-level commands.</li>
<li class=""><strong>Perception Feedback</strong>: Robot sensors provide real-time information to the LLM (e.g., &quot;fridge door is open,&quot; &quot;grasped bottle successfully&quot;).</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="prompt-engineering-for-robot-planning">Prompt Engineering for Robot Planning<a href="#prompt-engineering-for-robot-planning" class="hash-link" aria-label="Direct link to Prompt Engineering for Robot Planning" title="Direct link to Prompt Engineering for Robot Planning" translate="no">​</a></h2>
<p>Effectively utilizing LLMs for planning requires careful prompt engineering. The prompt given to the LLM needs to define:</p>
<ul>
<li class=""><strong>Robot&#x27;s Capabilities</strong>: A list of available actions/skills the robot can perform, along with their parameters and preconditions.</li>
<li class=""><strong>Environment State</strong>: Current information about the robot and its environment (e.g., known objects, robot&#x27;s location).</li>
<li class=""><strong>User&#x27;s Goal</strong>: The high-level natural language instruction.</li>
<li class=""><strong>Constraints</strong>: Any limitations or safety considerations.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-prompt-structure">Example Prompt Structure:<a href="#example-prompt-structure" class="hash-link" aria-label="Direct link to Example Prompt Structure:" title="Direct link to Example Prompt Structure:" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">You are a helpful robot assistant. Your goal is to generate a sequence of actions to fulfill human requests.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Available actions:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- move_to(location): Moves the robot to a specified location. Locations are: kitchen, living_room, fridge, table.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- grasp(object): Grasps an object. Objects are: cup, bottle.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- open(door_name): Opens a specified door. Doors are: fridge_door.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- close(door_name): Closes a specified door. Doors are: fridge_door.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Current environment state:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Robot is in the living_room.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Fridge is closed.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- Table has a cup.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Human Request: &quot;Please get me a bottle from the fridge.&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Generate a concise sequence of actions, one per line.</span><br></span></code></pre></div></div>
<p>The LLM might then respond with a sequence like:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">move_to(fridge)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">open(fridge_door)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">grasp(bottle)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">close(fridge_door)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">move_to(table)</span><br></span></code></pre></div></div>
<p>This sequence can then be parsed and executed by the ROS 2 Action Sequencer.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-in-llm-based-planning">Challenges in LLM-based Planning:<a href="#challenges-in-llm-based-planning" class="hash-link" aria-label="Direct link to Challenges in LLM-based Planning:" title="Direct link to Challenges in LLM-based Planning:" translate="no">​</a></h2>
<ul>
<li class=""><strong>Grounding</strong>: Ensuring the LLM&#x27;s abstract plan maps correctly to the robot&#x27;s physical capabilities and the real world.</li>
<li class=""><strong>Hallucinations</strong>: LLMs can generate plausible but physically impossible or incorrect plans.</li>
<li class=""><strong>Robustness</strong>: Handling unexpected events or changes in the environment that deviate from the LLM&#x27;s initial plan.</li>
<li class=""><strong>Safety</strong>: Ensuring generated plans are safe and do not lead to dangerous robot behaviors.</li>
<li class=""><strong>Computational Cost</strong>: Running large LLMs in real-time on robot hardware can be challenging.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion" translate="no">​</a></h2>
<p>LLM-based cognitive planning offers a paradigm shift in humanoid robotics, enabling robots to understand and execute complex natural language instructions. By acting as high-level planners, LLMs can decompose tasks, reason about the environment, and generate executable action sequences, bringing us closer to truly intelligent and adaptable humanoid agents. The next chapter will explore how these planned sequences are executed using ROS 2 action sequencing.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4-vla/chapter2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter1"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Voice-to-Action with Whisper: Capturing and Interpreting Commands</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/physical-ai-humanoid-robotics-textbook/docs/module4-vla/chapter3"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">ROS 2 Action Sequencing: Executing Planned Actions</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-need-for-cognitive-planning" class="table-of-contents__link toc-highlight">The Need for Cognitive Planning</a></li><li><a href="#llms-as-robot-planners" class="table-of-contents__link toc-highlight">LLMs as Robot Planners</a><ul><li><a href="#architecture-overview" class="table-of-contents__link toc-highlight">Architecture Overview:</a></li></ul></li><li><a href="#prompt-engineering-for-robot-planning" class="table-of-contents__link toc-highlight">Prompt Engineering for Robot Planning</a><ul><li><a href="#example-prompt-structure" class="table-of-contents__link toc-highlight">Example Prompt Structure:</a></li></ul></li><li><a href="#challenges-in-llm-based-planning" class="table-of-contents__link toc-highlight">Challenges in LLM-based Planning:</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics-textbook/docs/module1-ros2/chapter1">Textbook</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics-textbook/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Amnariazrit/physical-ai-humanoid-robotics-textbook" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics Textbook. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>